{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc2d7bd-f0b2-4012-84d6-006406ae707c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tejomaygadgil/cgpos/blob/nn/notebooks/colab/1_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b0da2-89d3-453f-aa3b-bfa840e42ea1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLVs6-8SdW16",
    "outputId": "e53f3e40-6f0f-47ff-c7bb-e3530b58a9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cgpos'...\n",
      "remote: Enumerating objects: 2199, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
      "remote: Total 2199 (delta 117), reused 127 (delta 71), pack-reused 2022\u001b[K\n",
      "Receiving objects: 100% (2199/2199), 101.85 MiB | 35.89 MiB/s, done.\n",
      "Resolving deltas: 100% (1363/1363), done.\n",
      "Collecting git+https://github.com/jtauber/greek-accentuation.git\n",
      "  Cloning https://github.com/jtauber/greek-accentuation.git to /tmp/pip-req-build-p2uil6ms\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/jtauber/greek-accentuation.git /tmp/pip-req-build-p2uil6ms\n",
      "  Resolved https://github.com/jtauber/greek-accentuation.git to commit 15ac5fd1cc82c8f9b91a4041f9b64399c9552097\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting beta-code\n",
      "  Downloading beta_code-1.1.0-py3-none-any.whl (8.4 kB)\n",
      "Building wheels for collected packages: greek-accentuation\n",
      "  Building wheel for greek-accentuation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for greek-accentuation: filename=greek_accentuation-1.2.0-py2.py3-none-any.whl size=7639 sha256=3410f14c88d242b2cabc2cb7bda2562c38940bedf852caa806018f9a11fbb605\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-84nt0gf8/wheels/31/68/a9/73949cf9e13b5f173b691a48aef7e9d728d04178209d0ae649\n",
      "Successfully built greek-accentuation\n",
      "Installing collected packages: beta-code, greek-accentuation\n",
      "Successfully installed beta-code-1.1.0 greek-accentuation-1.2.0\n",
      "/content/cgpos\n",
      "mkdir -p data/zip\n",
      "mkdir -p data/raw\n",
      "mkdir -p data/processed\n",
      "mkdir -p data/interim\n",
      "mkdir -p data/reference\n",
      "Initializing data directory\n"
     ]
    }
   ],
   "source": [
    "!git clone -b nn https://github.com/tejomaygadgil/cgpos.git\n",
    "!pip install git+https://github.com/jtauber/greek-accentuation.git beta-code\n",
    "%cd cgpos/\n",
    "!make process_ft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c286351-a1ab-48a3-ae69-c62c04d693d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea3725fb-b223-4501-b16e-b9694fbbf7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c976812-5ef1-4b6e-b427-efdf4bceb443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.model import Transformer\n",
    "import src.config as cfg\n",
    "from src.util import read_pkl, encode, decode, get_batch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43bf11e9-ccd0-4a00-8cab-9bd5fa14a20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_head = 8\n",
    "max_iters = 1000\n",
    "n_eval = 20\n",
    "\n",
    "params = {\n",
    "    \"train_size\": 0.98,  # Train params\n",
    "    \"n_chunks\": 500,\n",
    "    \"unc_rate\": 0.005,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",  # Device params\n",
    "    \"batch_size\": 64,  # Model hyperparameters\n",
    "    \"block_size\": 256,\n",
    "    \"n_head\": n_head,\n",
    "    \"emb_size\": 64 * n_head,\n",
    "    \"n_layer\": 6,\n",
    "    \"dropout\": 0.6,  # Training hyperparameters\n",
    "    \"max_iters\": max_iters,\n",
    "    \"eval_interval\": max_iters // n_eval,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"eval_iters\": 200,  # Monitor settings\n",
    "    \"generate_len\": 32,\n",
    "    \"torch_seed\": 20,  # Seeds\n",
    "    \"random_seed\": 40,\n",
    "    }\n",
    "\n",
    "for key, value in params.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "wandb.init(\n",
    "    project=\"ncgpos_ft\",\n",
    "    config=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c345b4ae-ae81-436e-bfa6-cbbd03544633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ft_syl = read_pkl(cfg.ft_syl)\n",
    "ft_targets = read_pkl(cfg.ft_targets)\n",
    "ft_targets_map = read_pkl(cfg.ft_targets_map)\n",
    "assert len(ft_syl) == len(ft_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf40f76c-c11e-4963-b09a-e3ed8d96ec7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "for i, word in enumerate(ft_syl):\n",
    "    for token in word:\n",
    "        tokens.append(token)\n",
    "        labels.append(ft_targets[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08d88270-5ba2-463e-aeed-bae16e4b3d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=17016, \n",
    "    block_size=256, \n",
    "    emb_size=512, \n",
    "    n_layer=6, \n",
    "    n_head=8, \n",
    "    dropout=0.6, \n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(torch.load(cfg.wts, map_location=torch.device(device)))\n",
    "# Omg!!\n",
    "model.lm_head = nn.Linear(512, len(ft_targets_map[1][0]))\n",
    "\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8db07a-9316-4357-88ad-b0050b295ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters, device, *batch_args):\n",
    "    out = []\n",
    "    model.eval()\n",
    "    for data in [train_data, val_data]:\n",
    "        losses = torch.zeros(eval_iters, device=device)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data, *batch_args)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out.append(losses.mean())\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78631f67-3350-4af0-8ec6-713ec0cc51a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=16970, bias=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "for step in tqdm(range(max_iters)):\n",
    "    # Evaluate training and val loss every eval_interval\n",
    "    if (step % eval_interval == 0) or (iter == max_iters - 1):\n",
    "        train_loss, val_loss = estimate_loss(\n",
    "            eval_iters, device, block_size, batch_size, device\n",
    "        )\n",
    "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "        with logging_redirect_tqdm():\n",
    "            logging.info(\n",
    "                f\"step {step}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\"\n",
    "            )\n",
    "            logging.info(generate(generate_len, block_size, model, device))\n",
    "\n",
    "    # Sample batch\n",
    "    xb, yb = get_batch(train_data, block_size, batch_size, device)\n",
    "    _, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a319f13-c2a1-42db-9225-9335d253e698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
