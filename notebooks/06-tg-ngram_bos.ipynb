{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb314d06-f20c-4e2a-8f68-ebcc7b9e75e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3eea84-3019-4ce1-bd46-6fc758323db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "from greek_accentuation.characters import strip_accents, strip_breathing\n",
    "\n",
    "from cgpos.utils.util import import_pkl, export_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945faf51-fbe6-4be0-a2be-8f42843ccc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "# Load hydra params\n",
    "initialize(\"../conf/\", version_base=None)\n",
    "config = compose(config_name='main')\n",
    "# Init logger\n",
    "logging.basicConfig(level=logging.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2405ed92-b0b4-4102-a594-1640227d2762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cgpos.utils.util:Importing /home/tejomay/cgpos/data/processed/perseus_featurized.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_raw = import_pkl(config.perseus.featurized)\n",
    "# Clean\n",
    "data = [word for word in data_raw if 'pos' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab092cb-d4ec-418e-b081-c3fc31adb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(sequence, n):\n",
    "    \"\"\"\n",
    "    Generate ngram from a sequence of tokens.\n",
    "    \"\"\"\n",
    "    len_sequence = len(sequence)\n",
    "    n_passes = max(1, len_sequence - n + 1)\n",
    "    return (tuple(sequence[i:(i + n)]) for i in range(n_passes))\n",
    "\n",
    "def ngram_bos(data, n=1):\n",
    "    \"\"\"\n",
    "    Generate n-gram bag of syllables for given data.\n",
    "    \"\"\"\n",
    "    bos = defaultdict(Counter)\n",
    "    for word in data:\n",
    "        pos = word['pos']\n",
    "        syllables = word['syllables']\n",
    "        for gram in ngram(syllables, n):\n",
    "            bos[gram][pos] +=1 \n",
    "    return bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "37807e5e-d0a3-41f6-9c88-10caa589bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of syllables\n",
    "def ngram_bos_model(data, n=2, train=0.8, verbose=True, seed=20):\n",
    "    \"\"\"\n",
    "    Implement n-gram bag of syllable model.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"N-gram bag of syllables model: n={n}\")\n",
    "    \n",
    "    # Train test split\n",
    "    random.seed(seed)\n",
    "    shuffled = random.sample(data, len(data))\n",
    "    train_ind = int(len(data) * train)\n",
    "    train_data = shuffled[:train_ind] \n",
    "    test_data = shuffled[train_ind:]\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Train-test split {train}: (train={len(train_data)}, test={len(test_data)}) [seed={seed}]\")\n",
    "\n",
    "    # Train\n",
    "    train_grams = {}\n",
    "    for i in range(1, n + 1):\n",
    "        train_gram = ngram_bos(train_data, n=i)\n",
    "        train_grams[i] = train_gram\n",
    "\n",
    "    # Test\n",
    "    unseen_grams = []\n",
    "    for word in test_data:\n",
    "        gram_preds = []\n",
    "        word_grams = ngram(word['syllables'], n)\n",
    "        for gram in word_grams:\n",
    "            if (gram in train_grams[n]):\n",
    "                gram_dist = train_grams[n][gram]\n",
    "                gram_pred = max(gram_dist, key=gram_dist.get)    \n",
    "                gram_preds.append(gram_pred)\n",
    "            else:\n",
    "                subgrams = ngram(gram, n - 1)\n",
    "                for subgram in subgrams:\n",
    "                    subgram_preds = []\n",
    "                    if (subgram in train_grams[n - 1]):\n",
    "                        subgram_dist = train_grams[n - 1][subgram]\n",
    "                        subgram_pred = max(subgram_dist, key=subgram_dist.get)\n",
    "                        subgram_preds.append(subgram_pred)\n",
    "                    else:\n",
    "                        subsubgrams = ngram(subgram, n - 2)\n",
    "                        for subsubgram in subsubgrams:\n",
    "                            subsubgram_preds = []\n",
    "                            if (subsubgram in train_grams[n - 2]):\n",
    "                                subsubgram_dist = train_grams[n - 2][subsubgram]\n",
    "                                subsubgram_pred = max(subsubgram_dist, key=subsubgram_dist.get)\n",
    "                                subsubgram_preds.append(subsubgram_pred)\n",
    "                        if subsubgram_preds:\n",
    "                            subsubgram_pos = max(set(subsubgram_preds), key=subsubgram_preds.count)\n",
    "                            subgram_preds.append(subsubgram_pos)\n",
    "                if subgram_preds:\n",
    "                    subgram_pos = max(set(subgram_preds), key=subgram_preds.count)\n",
    "                    gram_preds.append(subgram_pos)\n",
    "                else:\n",
    "                    unseen_grams.append(gram)\n",
    "                # unseen_grams.append(gram)\n",
    "        if gram_preds:\n",
    "            pos_pred = max(set(gram_preds), key=gram_preds.count)\n",
    "        else:\n",
    "            pos_pred = None\n",
    "        word['pos_pred'] = pos_pred\n",
    "    \n",
    "    accuracy = sum([(word['pos'] == word['pos_pred']) for word in test_data]) / len(test_data)\n",
    "    \n",
    "    if verbose:\n",
    "        logger.info(f\"Accuracy: {accuracy * 100:.2f}%, unseen grams: {len(unseen_grams)}\")\n",
    "\n",
    "    results = {\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'train_grams': train_grams,\n",
    "        'unseen_grams': unseen_grams,\n",
    "        'accuracy': accuracy,\n",
    "        'seed': seed,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def cv(data, model=ngram_bos_model, folds=10, **kwargs):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Running {folds}-fold CV on n-gram bag-of-syllables model:\")\n",
    "    results = []\n",
    "    accuracies = []\n",
    "    for i in range(1, folds + 1):\n",
    "        logger.info(f\"Training fold {i}:\")\n",
    "        seed = random.randint(1, 2**10)\n",
    "        result = model(data, seed=seed, **kwargs)\n",
    "        accuracy = result['accuracy']\n",
    "        results.append(result)\n",
    "        accuracies.append(accuracy)\n",
    "    logger.info(f\"{folds}-fold CV results: accuracy {np.mean(accuracies) * 100:.2f}%, std {np.std(accuracies):.2f}\")\n",
    "    accuracies.append(accuracy)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "abac6d9d-f2d0-4d1c-a6d8-2aa5155bac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:N-gram bag of syllables model: n=3\n",
      "INFO:__main__:Train-test split 0.8: (train=439916, test=109980) [seed=20]\n",
      "INFO:__main__:Accuracy: 90.24%, unseen grams: 298\n"
     ]
    }
   ],
   "source": [
    "results = ngram_bos_model(data, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c17a6-2115-4ee5-ad23-54913e20f957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcbcc952-6edd-4ff9-9846-68097e738ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try removing accents\n",
    "data_no_accent = copy.deepcopy(data)\n",
    "for word in data_no_accent:\n",
    "    if 'syllables' in word:\n",
    "        word['syllables'] = [strip_breathing(strip_accents(syllable)) for syllable in word['syllables']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3bdbf351-9c2d-4442-aed4-be48c7118ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Θου', 'κυ', 'δί', 'δης'],\n",
       " ['Ἀ', 'θη', 'ναῖ', 'ος'],\n",
       " ['ξυ', 'νέ', 'γρα', 'ψε'],\n",
       " ['τὸν'],\n",
       " ['πό', 'λε', 'μον'],\n",
       " ['τῶν'],\n",
       " ['Πε', 'λο', 'πον', 'νη', 'σί', 'ων'],\n",
       " ['καὶ'],\n",
       " ['Ἀ', 'θη', 'ναί', 'ων'],\n",
       " [',']]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word['syllables'] for word in data][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c9d5fc32-1f55-4391-a25e-e947b2d74d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Θου', 'κυ', 'δι', 'δης'],\n",
       " ['Α', 'θη', 'ναι', 'ος'],\n",
       " ['ξυ', 'νε', 'γρα', 'ψε'],\n",
       " ['τον'],\n",
       " ['πο', 'λε', 'μον'],\n",
       " ['των'],\n",
       " ['Πε', 'λο', 'πον', 'νη', 'σι', 'ων'],\n",
       " ['και'],\n",
       " ['Α', 'θη', 'ναι', 'ων'],\n",
       " [',']]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word['syllables'] for word in data_no_accent][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "227612e0-93a3-478a-b0fb-b28ee02e8ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running 1-fold CV on n-gram bag-of-syllables model:\n",
      "INFO:__main__:Training fold 1:\n",
      "INFO:__main__:N-gram bag of syllables model: n=3\n",
      "INFO:__main__:Train-test split 0.8: (train=439916, test=109980) [seed=565]\n",
      "INFO:__main__:Accuracy: 88.93%, unseen grams: 129\n",
      "INFO:__main__:1-fold CV results: accuracy 88.93%, std 0.00\n"
     ]
    }
   ],
   "source": [
    "_ = cv(data_no_accent, folds=1, n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
